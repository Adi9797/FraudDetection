{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some short reviews of the most popular notebooks from the Kaggle competition: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>Each reviewed notebook will have the following structure:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook link: https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short description of the goals of the notebook: dataset exploration, dataset classification (LogisticRegression, SVC, KNeighborsClassifier, DecisionTreeClassifier, RandomForestClassifier), NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS:- he created a dataset where the amount of fraudulent and non-fraudulent transaction were the same = balanced dataset\n",
    "    - removed (extreme) outliers: Interquartile Range Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method: t-SNE= clustering algorithm;\n",
    "        PCA\n",
    "        SVD\n",
    "pros; cons;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method: - logistic regression:  more accurate than the other three classifiers in most cases\n",
    "        - KNN\n",
    "        - SVC\n",
    "        - decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: SMOTE = Synthetic Minority Over-sampling Technique. Unlike Random UnderSampling, SMOTE creates new synthetic points in order to have an equal balance of the classes. This is another alternative for solving the \"class imbalance problems\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method: all of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method: NN\n",
    "        Neural Network Structure: As stated previously, this will be a simple model composed of one input layer (where the number of nodes equals the number of features) plus bias node, one hidden layer with 32 nodes and one output node composed of two possible results 0 or 1 (No fraud or fraud).\n",
    "Other characteristics: The learning rate will be 0.001, the optimizer we will use is the AdamOptimizer, the activation function that is used in this scenario is \"Relu\" and for the final outputs we will use sparse categorical cross entropy, which gives the probability whether an instance case is no fraud or fraud (The prediction will pick the highest probability between the two.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook link: https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short description of the goals of the notebook: classif on skewed data. Hyperparam tuning on different models (Logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are several ways to approach this classification problem taking into consideration this unbalance. \n",
    "\n",
    "- Collect more data? Nice strategy but not applicable in this case\n",
    "- Changing the performance metric:\n",
    "    - Use the confusio nmatrix to calculate Precision, Recall\n",
    "    - F1score (weighted average of precision recall)\n",
    "    - Use Kappa - which is a classification accuracy normalized by the imbalance of the classes in the data\n",
    "    - ROC curves - calculates sensitivity/specificity ratio.\n",
    "- Resampling the dataset\n",
    "    - Essentially this is a method that will process the data to have an approximate 50-50 ratio.\n",
    "    - One way to achieve this is by OVER-sampling, which is adding copies of the under-represented class (better when you have little data)\n",
    "    - Another is UNDER-sampling, which deletes instances from the over-represented class (better when he have lot's of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- undersample appiled\n",
    "- precision/recall compromise\n",
    "- 93.2% recall accuracy measure on the undersampled test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "<br>Semi Supervised Classification using AutoEncoders\n",
    "<br>Notebook link: https://www.kaggle.com/shivamb/semi-supervised-classification-using-autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs:\n",
    "- keras was used\n",
    "- the target is highly imbalanced as only 0.17 %\n",
    "- more about autoencoders: https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook link: https://www.kaggle.com/pavansanagapati/automated-hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- good as inspiration for the hyperparam tuning part of the process: Model-Free (GridSearch vs RandomSearch); Model_based(bayesian optimization, non-probabilistic, evolutionary algos)\n",
    "<br>\n",
    "- Bayesian Optimization can be performed in Python using the Hyperopt library.\n",
    "<br>\n",
    "- Genetic Algorithms in Python, we can use the TPOT Auto Machine Learning library. TPOT is built on the scikit-learn library and it can be used for either regression or classification tasks.\n",
    "<br>\n",
    "- optuna\n",
    "<br>\n",
    "- tune\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook link: https://www.kaggle.com/gpreda/credit-card-fraud-detection-predictive-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used tree-based models:\n",
    "- RandomForrestClassifier\n",
    "- AdaBoostClassifier\n",
    "- CatBoostClassifier\n",
    "- XGBoost\n",
    "- LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "\n",
    "Notebook link https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve\n",
    "\n",
    "Receiver Operating Characteristics Curve (ROC Curve)vs Precision/Recall curve\n",
    "\n",
    "Obs:\n",
    "- Hour of the day seems to have some impact on the number of Fraud cases.\n",
    "- LogisticRegression, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "- 0.59 recall It seems like we aren't very good with catching our frauds, which is expected with a vanilla Logistic Regression without addressing the class imbalance issue.\n",
    "- Sci-Kit Learn classifiers can give heavier weights to the minority class using a simple parameter during model initiation. Let's see how that will improve our results=> 92% True Positive rate\n",
    "\n",
    "- plot ROC/PR curve - Whereas Precision ( True Positives / (True Positives + False Positives) ) is highly sensitive to False Positives and is not impacted by a large total real negative denominator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook link: https://www.kaggle.com/qianchao/smote-with-imbalance-data\n",
    "\n",
    "- meh, SMOTE, linear regression, ROC curve, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook link: https://www.kaggle.com/gargmanish/how-to-handle-imbalance-data-study-in-detail\n",
    "\n",
    "- SMOTE\n",
    "- Decision Tree Classifier/ Random Forest Classifier<br>\n",
    "  Logistic regression<br>\n",
    "  SVM - A better recall but precision is not improving much<br>\n",
    "  XGboost\n",
    "- percentage of normal transacation is 99.82725143693798<br>\n",
    "  percentage of fraud transacation 0.1727485630620034\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my previous version I got the 100 recall and 98 % precision by using Random forest with the over sampled data but in real it was due to over fitting because i was taking whole fraud data and was training for that and I was doing the testing on the same data.\n",
    "\n",
    "- The recall is nearby the previous one done by over sampling\n",
    "The precision decrease in this case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook link: https://www.kaggle.com/residentmario/undersampling-and-oversampling-imbalanced-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
